convert.magic <- function(obj,types){
    for (i in 1:length(obj)){
        FUN <- switch(types[i],character = as.character, 
                                   numeric = as.numeric, 
                                   factor = as.factor)
        obj[,i] <- FUN(obj[,i])
    }
    obj
}

heart.data <- read.csv("cleveland_data.csv",header=FALSE,sep=",",na.strings = '?')
names(heart.data) <- c( "age", "sex", "cp", "trestbps", "chol","fbs", "restecg","thalach","exang", "oldpeak","slope", "ca", "thal", "num")
head(heart.data,10)
dim(heart.data)
	
							#how many had heart attack, women or men, age?

heart.data$num[heart.data$num > 0] <- 1

----------------------------------------------------------------

barplot(table(heart.data$num),main="Fate", col="black")

chclass <-c("numeric","factor","factor","numeric","numeric","factor","factor","numeric","factor","numeric","factor","factor","factor","factor")
heart.data <- convert.magic(heart.data,chclass)
heart = heart.data 
#add labels only for plot
levels(heart$num) = c("No disease","Disease")
levels(heart$sex) = c("female","male","")
mosaicplot(heart$sex ~ heart$num,main="Fate by Gender", shade=FALSE,color=TRUE,xlab="Gender", ylab="Heart disease")

boxplot(heart$age ~ heart$num,main="Fate by Age", ylab="Age",xlab="Heart disease")

						#Check for missing values - only 6 so just remove them.
s = sum(is.na(heart.data))
heart.data <- na.omit(heart.data)

							    # Training and Testing data 70-30 %

library(caret)
set.seed(10)
inTrainRows <- createDataPartition(heart.data$num,p=0.7,list=FALSE)
trainData <- heart.data[inTrainRows,]
testData <-  heart.data[-inTrainRows,]

nrow(trainData)/(nrow(testData)+nrow(trainData)) #check the division

AUC = list()
Accuracy = list()

								#Logistic Regression

set.seed(10)
logRegModel <- train(num ~ ., data=trainData, method = 'glm', family = 'binomial')
logRegPrediction <- predict(logRegModel, testData)
logRegPredictionprob <- predict(logRegModel, testData, type='prob')[2]
logRegConfMat <- confusionMatrix(logRegPrediction, testData[,"num"])
#ROC Curve
library(pROC)
AUC$Log_Reg <- roc(as.numeric(testData$num),as.numeric(as.matrix((logRegPredictionprob))))$auc
Accuracy$Log_Reg <- logRegConfMat$overall['Accuracy'] 

								#Support Vector Machines

# training sets used in a variety of domains are not linearly separated so that it is hard to derive the optimal hyperplane correctly classifying vectors in two # classes. To solve this non-linearity problem, several solutions called kernel functions have been proposed and adopted for SVM.
# Radial basis function (RBF): K(xi, xj) = exp(-??xi - xj?2), ? > 0

feature.names=names(heart.data)
for (f in feature.names) {
  if (class(heart.data[[f]])=="factor") {
    levels <- unique(c(heart.data[[f]]))
    heart.data[[f]] <- factor(heart.data[[f]],
                   labels=make.names(levels))
  }
}
inTrainRows <- createDataPartition(heart.data$num,p=0.7,list=FALSE)
trainData2 <- heart.data[inTrainRows,]
testData2 <-  heart.data[-inTrainRows,]
fitControl <- trainControl(method = "repeatedcv",number = 10,repeats = 10,classProbs = TRUE,summaryFunction = twoClassSummary)

set.seed(10)
svmModel <- train(num ~ ., data = trainData2,method = "svmRadial",trControl = fitControl,preProcess = c("center", "scale"),tuneLength = 8,metric = "ROC")
svmPrediction <- predict(svmModel, testData2)
svmPredictionprob <- predict(svmModel, testData2, type='prob')[2]
svmConfMat <- confusionMatrix(svmPrediction, testData2[,"num"])
#ROC Curve
AUC$SVM <- roc(as.numeric(testData2$num),as.numeric(as.matrix((svmPredictionprob))))$auc
Accuracy$SVM <- svmConfMat$overall['Accuracy'] 

								#ACCURACY

# AUC -> area under the ROC which represents the proportion of true positive and the proportion of false positive. 
# Accuracy -> true positive and true negative divided by all results.

row.names <- names(Accuracy)
col.names <- c("AUC", "Accuracy")
cbind(as.data.frame(matrix(c(AUC,Accuracy),nrow = 2, ncol = 2,dimnames = list(row.names, col.names))))

								# Interpretation of Logistic Regression Model
summary(logRegModel)$coeff 

# The interpretation of the coefficient for sex, for example, is: If all predictors are held at a fixed value, the odds of getting heart disease for males (males = 1) # over the odds of getting heart disease for females is exp(1.204747309) = 3.336 i.e. the odds are 7693% higher.

								# Confusion Matrix

logRegConfMat
svmConfMat
